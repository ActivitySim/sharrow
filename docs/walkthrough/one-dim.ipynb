{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Sharrow Basics\n",
    "\n",
    "This notebook provides a short walkthrough of some of the basic features of the `sharrow` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "\n",
    "import numba as nb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import sharrow as sh\n",
    "\n",
    "sh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# check versions\n",
    "import packaging\n",
    "\n",
    "assert packaging.version.parse(xr.__version__) >= packaging.version.parse(\"0.20.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "\n",
    "We'll begin by importing some example data to work with.  We'll be using \n",
    "some test data taken from the MTC example in the ActivitySim project, including \n",
    "tables of data for households and persons, as well as a set of \n",
    "skims containing transportation level of service information for travel around\n",
    "a tiny slice of San Francisco.\n",
    "\n",
    "The households and persons are typical tabular data, and \n",
    "each can be read in and stored as a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "households = sh.example_data.get_households()\n",
    "households.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST households content\n",
    "assert len(households) == 5000\n",
    "assert \"income\" in households\n",
    "assert households.index.name == \"HHID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = sh.example_data.get_persons()\n",
    "persons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert len(persons) == 8212\n",
    "assert \"household_id\" in persons\n",
    "assert persons.index.name == \"PERID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The skims, on the other hand, are not just simple tabular data, but rather a \n",
    "multi-dimensional representation of the transportation system, indexed by origin.\n",
    "destination, and time of day. Rather than using a single DataFrame for this data,\n",
    "we store it as a multi-dimensional `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims = sh.example_data.get_skims()\n",
    "skims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "For tabular data, sharrow can be provided either pandas DataFrames or xarray Datasets, \n",
    "but to ensure consistency the former are converted into the latter automatically when\n",
    "they are used with sharrow.  You can also easily manually make the conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.Dataset(persons)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Suppose we're wanting to simulate a tour mode choice.  Normally we'd probably have\n",
    "run through a bunch of different models to generate these tours and their destinations\n",
    "first, but let's just skip that for now and make up some random data to work with.  We'll \n",
    "just randomly choose (with replacement) 100,000 people, and send them to 100,000 zones, with\n",
    "random outbound and inbound time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tours(n_tours=100_000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_zones = skims.dims[\"dtaz\"]\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"PERID\": rng.choice(persons.index, size=n_tours),\n",
    "            \"dest_taz_idx\": rng.choice(n_zones, size=n_tours),\n",
    "            \"out_time_period\": rng.choice(skims.time_period, size=n_tours),\n",
    "            \"in_time_period\": rng.choice(skims.time_period, size=n_tours),\n",
    "        }\n",
    "    ).rename_axis(\"TOURIDX\")\n",
    "\n",
    "\n",
    "tours = random_tours()\n",
    "tours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert tours.index.name == \"TOURIDX\"\n",
    "assert 0 in tours.head().dest_taz_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Of note in this table, we include include destination TAZ's by index (position) not \n",
    "label, so we can observe a TAZ index of `0` even though the first TAZ ID is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Spec Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Now that we've got our tours to work with, we'll also need \n",
    "an expression \"spec\" file that defines the utility function\n",
    "terms and coefficients.  Following the ActivitySim format, we\n",
    "can write a mini-spec file as appears below.  Each line of this\n",
    "CSV file has an expression that can be evaluated in the context\n",
    "of the various tables and datasets shown above, plus a set of \n",
    "coefficients that apply for that expression across various modal \n",
    "alternatives (drive, walk, and transit in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_spec = \"\"\"\n",
    "Label,Expression,DRIVE,WALK,TRANSIT\n",
    "Drive Time,odt_skims['SOV_TIME'] + dot_skims['SOV_TIME'],-0.0134,,\n",
    "Transit IVT,(odt_skims['WLK_LOC_WLK_TOTIVT']/100 + dot_skims['WLK_LOC_WLK_TOTIVT']/100),,,-0.0134\n",
    "Transit Wait Time,short_i_wait_mult * ((odt_skims['WLK_LOC_WLK_IWAIT']/100).clip(upper=shortwait) + (dot_skims['WLK_LOC_WLK_IWAIT']/100).clip(upper=shortwait)),,,-0.0134\n",
    "Income,hh.income > income_breakpoints[2],,-0.2,\n",
    "Constant,one,,-0.4,-0.55\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "We'll use pandas to load these values into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = pd.read_csv(StringIO(mini_spec), index_col=\"Label\")\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST check spec\n",
    "assert spec.index.name == \"Label\"\n",
    "assert all(spec.columns == [\"Expression\", \"DRIVE\", \"WALK\", \"TRANSIT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Data Trees and Flows\n",
    "\n",
    "Then, it's time to prepare our data.  We'll create a `DataTree`\n",
    "that defines the relationships among all the datasets we're working\n",
    "with.  This is a tree in the mathematical sense, with nodes referencing\n",
    "the datasets and edges representing the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_breakpoints = nb.typed.Dict.empty(nb.types.int32, nb.types.int32)\n",
    "income_breakpoints[0] = 15000\n",
    "income_breakpoints[1] = 30000\n",
    "income_breakpoints[2] = 60000\n",
    "\n",
    "tree = sh.DataTree(\n",
    "    tour=tours,\n",
    "    person=persons,\n",
    "    hh=households,\n",
    "    odt_skims=skims,\n",
    "    dot_skims=skims,\n",
    "    relationships=(\n",
    "        \"tour.PERID @ person.PERID\",\n",
    "        \"person.household_id @ hh.HHID\",\n",
    "        \"hh.TAZ @ odt_skims.otaz\",\n",
    "        \"tour.dest_taz_idx -> odt_skims.dtaz\",\n",
    "        \"tour.out_time_period @ odt_skims.time_period\",\n",
    "        \"tour.dest_taz_idx -> dot_skims.otaz\",\n",
    "        \"hh.TAZ @ dot_skims.dtaz\",\n",
    "        \"tour.in_time_period @ dot_skims.time_period\",\n",
    "    ),\n",
    "    extra_vars={\n",
    "        \"shortwait\": 3,\n",
    "        \"one\": 1,\n",
    "    },\n",
    "    aux_vars={\n",
    "        \"short_i_wait_mult\": 0.75,\n",
    "        \"income_breakpoints\": income_breakpoints,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "The first named dataset we include, `tour`, is by default the root node of this data tree.\n",
    "We then can define an arbitrary number of other named data nodes.  Here, we add `person`, `hh`,\n",
    "`odt_skims` and `odt_skims`.  Note that these last two are actually two different names for the\n",
    "same underlying dataset, and for each name we will next define a unique set of relationships.\n",
    "\n",
    "All data nodes in this tree are stored as `Dataset` objects. We can give a pandas DataFrame\n",
    "in this contructor instead, but it will be automatically converted into a one-dimension `Dataset`.\n",
    "The conversion is no-copy if possible (and it is usually possible) so no additional memory is\n",
    "consumed in the conversion.\n",
    "\n",
    "The `relationships` defines links of the data tree. Each relationship maps a particular variable\n",
    "in a named upstream dataset to a particular dimension of a named downstream dataset.  For example,\n",
    "`\"person.household_id @ hh.HHID\"` tells the tree that the `household_id` variable in the `person` \n",
    "dataset contains labels (`@`) that map to the `HHID` dimension of the `hh` dataset.\n",
    "\n",
    "In addition to mapping by label, we can also map by position, by using the `->` operator in the\n",
    "relationship string instead of `@`.  In the example above, we map the tour destination TAZ's in\n",
    "this manner, as the `dest_taz_idx` variable in the `tours` dataset contains positional references\n",
    "instead of labels.\n",
    "\n",
    "A special case for the relationship mapping is available when the source varibable\n",
    "in the upstream dataset is explicitly categorical.  In this case, sharrow checks that\n",
    "the categories exactly match the labels in the referenced downstream dataset dimension,\n",
    "and that there are no missing categorical values. If they do match and there are no\n",
    "missing values, the code points of the categories are used as positional mapping\n",
    "references, which is both memory and runtime efficient.  If they *don't* match, an\n",
    "error is raised, as it is presumed that the user has made a mistake... in theory \n",
    "sharrow could unravel the category values and do the mapping by label, but this would\n",
    "be a cumbersome operation, contrary to the efficiency goals of the library.\n",
    "\n",
    "Lastly, our tree definition includes a few named constants, that are just fixed values defined\n",
    "in a separate dictionary. These are shown in two groups, `extra_vars` and `aux_vars`. The values \n",
    "in `extra_vars` get hard-coded into the compiled results, effectively the \n",
    "same as if their values were expanded and written into exprssions in the `spec` directly. This is\n",
    "generally most efficient if the values will never change.  On the other hand, `aux_vars` will be \n",
    "passed by reference into the compiled results. These values need to be numba-safe objects, so\n",
    "for instance a regular Python dictionary can't be used, but a numba typed Dict is acceptable.\n",
    "So long as the data type and dimensionality of the values in `aux_vars` remains constant, the \n",
    "actual values can be changed later (i.e. after compilation).\n",
    "\n",
    "Once we have defined our data tree, we can use it along with the `spec`, to compute the utility\n",
    "for various alternatives in the choice model.  Sharrow allows us to compile this utility function\n",
    "into a `Flow`, which can be reused for massive speed gains on later utility evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = tree.setup_flow(spec.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "To use a `Flow` for preparing the array of data that backs the utility\n",
    "function, we can call the `load()` method. The first time we call `load()`,\n",
    "it takes a (relatively) long time to evaluate, as the expressions are compiled\n",
    "and that compiled code is cached to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "assert flow.tree.aux_vars[\"short_i_wait_mult\"] == 0.75\n",
    "assert flow.tree.aux_vars[\"income_breakpoints\"][2] == 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST utility data\n",
    "assert flow.check_cache_misses(fresh=False)\n",
    "actual = flow.load()\n",
    "expected = np.array(\n",
    "    [\n",
    "        [9.4, 16.9572, 4.5, 0.0, 1.0],\n",
    "        [9.32, 14.3628, 4.5, 1.0, 1.0],\n",
    "        [7.62, 11.0129, 4.5, 1.0, 1.0],\n",
    "        [4.25, 7.6692, 2.50065, 0.0, 1.0],\n",
    "        [6.16, 8.2186, 3.387825, 0.0, 1.0],\n",
    "        [4.86, 4.9288, 4.5, 0.0, 1.0],\n",
    "        [1.07, 0.0, 0.0, 0.0, 1.0],\n",
    "        [8.52, 11.615499, 3.260325, 0.0, 1.0],\n",
    "        [11.74, 16.2798, 3.440325, 0.0, 1.0],\n",
    "        [10.48, 13.3974, 3.942825, 0.0, 1.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(actual[:5], expected[:5])\n",
    "np.testing.assert_array_almost_equal(actual[-5:], expected[-5:])\n",
    "assert actual.shape == (len(tours), len(spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Subsequent calls to `load()` are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST compile flags\n",
    "flow.load(compile_watch=False)\n",
    "import pytest\n",
    "\n",
    "with pytest.raises(AttributeError):\n",
    "    compiled_recently = (\n",
    "        flow.compiled_recently\n",
    "    )  # attribute does not exist if compile_watch flag is off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "It's not faster because it's cached the data, but because it's cached the compiled code.\n",
    "(Setting the `compile_watch` argument to a truthy value will trigger a check of the \n",
    "cache files and emit a warning message if recompilation was triggered.)\n",
    "We can swap out the `tour` node in the tree for a different set of (similarly formatted)\n",
    "tours, and re-evaluate at that fast speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tours_2 = random_tours(seed=43)\n",
    "tours_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Note that the flow requires not just a base dataset but a whole DataTree to operate,\n",
    "so to re-evaluate with a new `tours` we need to make a DataTree with `replace_datasets`.\n",
    "Fortuntately, this operation is no-copy so it doesn't consume much memory.  If all the \n",
    "datasets in a tree are linked by position (instead of by label) this would be almost \n",
    "instantaneous, but since our example tree here has tours linked by label it takes just a\n",
    "moment to rebuild the linkages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_2 = tree.replace_datasets(tour=tours_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "from pytest import approx\n",
    "\n",
    "assert tree_2.aux_vars[\"short_i_wait_mult\"] == 0.75\n",
    "assert tree_2.aux_vars[\"income_breakpoints\"][2] == approx(60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow.load(tree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST that aux_vars also work with arrays\n",
    "tree_a = tree_2.replace_datasets(tour=tours)\n",
    "tree_a.aux_vars[\"income_breakpoints\"] = np.asarray([1, 2, 60000])\n",
    "actual = flow.load(tree_a)\n",
    "expected = np.array(\n",
    "    [\n",
    "        [9.4, 16.9572, 4.5, 0.0, 1.0],\n",
    "        [9.32, 14.3628, 4.5, 1.0, 1.0],\n",
    "        [7.62, 11.0129, 4.5, 1.0, 1.0],\n",
    "        [4.25, 7.6692, 2.50065, 0.0, 1.0],\n",
    "        [6.16, 8.2186, 3.387825, 0.0, 1.0],\n",
    "        [4.86, 4.9288, 4.5, 0.0, 1.0],\n",
    "        [1.07, 0.0, 0.0, 0.0, 1.0],\n",
    "        [8.52, 11.615499, 3.260325, 0.0, 1.0],\n",
    "        [11.74, 16.2798, 3.440325, 0.0, 1.0],\n",
    "        [10.48, 13.3974, 3.942825, 0.0, 1.0],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(actual[:5], expected[:5])\n",
    "np.testing.assert_array_almost_equal(actual[-5:], expected[-5:])\n",
    "assert actual.shape == (len(tours), len(spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "The load function also has some other features, like nicely formatting the output\n",
    "into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flow.load_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST df\n",
    "assert len(df) == len(tours)\n",
    "pd.testing.assert_index_equal(\n",
    "    df.columns,\n",
    "    pd.Index([\"Drive Time\", \"Transit IVT\", \"Transit Wait Time\", \"Income\", \"Constant\"]),\n",
    ")\n",
    "expected_df_head = pd.read_csv(\n",
    "    StringIO(\n",
    "        \"\"\",Drive Time,Transit IVT,Transit Wait Time,Income,Constant\n",
    "0,9.4,16.9572,4.5,0.0,1.0\n",
    "1,9.32,14.3628,4.5,1.0,1.0\n",
    "2,7.62,11.0129,4.5,1.0,1.0\n",
    "3,4.25,7.6692,2.50065,0.0,1.0\n",
    "4,6.16,8.2186,3.387825,0.0,1.0\"\"\"\n",
    "    ),\n",
    "    index_col=0,\n",
    ").astype(np.float32)\n",
    "pd.testing.assert_frame_equal(df.head(), expected_df_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Linear-in-Parameters Functions\n",
    "\n",
    "When the `spec` represents a linear-in-parameters utility function, the data \n",
    "we get out of the `load()` function represents one matrix in a dot-product, and\n",
    "the coefficients in the `spec` provide the other matrix.  We might look to \n",
    "use the efficient linear algebra algorithms embedded in `np.dot` to compute the\n",
    "utility, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flow.load()\n",
    "b = spec.iloc[:, 1:].fillna(0).astype(np.float32).values\n",
    "np.dot(x, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "But `sharrow` provides a substantially faster option, by embedding\n",
    "the dot product directly into the compiled code and never instantiating the\n",
    "full `x` array in memory at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = flow.dot(b)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST utility\n",
    "np.testing.assert_array_almost_equal(u, np.dot(x, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "As before, the compiler runs only the first time we apply the this \n",
    "function with this structure, and subsequent runs are faster, even with\n",
    "different source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow.dot(b, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "As for the plain `load` method, the `dot` method also has some formatted output versions.\n",
    "For example, the `dot_dataarray` returns a `DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.dot_dataarray(b, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "This works even better if the coefficients are given as a DataArray too, so it \n",
    "can harvest dimension names and coordinates as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = xr.DataArray(\n",
    "    spec.iloc[:, 1:].fillna(0).astype(np.float32), dims=(\"expressions\", \"modes\")\n",
    ")\n",
    "flow.dot_dataarray(B, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Multinomial Logit Simulation\n",
    "\n",
    "The next level of flow evaluation is made by treating the dot-product as a\n",
    "linear-in-parameters multinomial logit (MNL) utility function, and making simulated\n",
    "choices based on that model.  To do this, we'll need to provide the random\n",
    "draws as a function input (which also lets us attach any randomization engine\n",
    "we prefer, e.g. a reproducible random generator).  For this example, we'll \n",
    "create one random (uniform) draw for each tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = np.random.default_rng(321).random(size=tree.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "Given those draws, we use the `logit_draws` method to build and apply a \n",
    "MNL simulator, which returns to us both the choices and the probability that\n",
    "was computed for each chosen alternative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices, choice_probs = flow.logit_draws(b, draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time choices, choice_probs = flow.logit_draws(b, draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "As this is the most complex flow processor,\n",
    "it takes the longest to compile, but after compilation it runs quite efficiently.\n",
    "We can see here the whole MNL simulation process for this data requires only a few \n",
    "milliseconds more time than just computing the utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices2, choice_probs2 = flow.logit_draws(b, draws, source=tree_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time choices2, choice_probs2 = flow.logit_draws(b, draws, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "The resulting choices are the index position of the choices, not the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "But if we want the labels, it's easy enough to convert these indexes into labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "B.modes[choices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST mnl choices\n",
    "uz = np.exp(flow.dot(b))\n",
    "uz = uz / uz.sum(1)[:, None]\n",
    "np.testing.assert_array_almost_equal(\n",
    "    uz[range(uz.shape[0]), choices.ravel()],\n",
    "    choice_probs.ravel(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "choices_darr, choice_probs_darr = flow.logit_draws(b, draws, as_dataarray=True)\n",
    "assert choices_darr.dims == (\"TOURIDX\",)\n",
    "assert choices_darr.shape == (100000,)\n",
    "assert choice_probs_darr.dims == (\"TOURIDX\",)\n",
    "assert choice_probs_darr.shape == (100000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "## Nested Logit Simulation\n",
    "\n",
    "Sharrow can also apply nested logit models.  To do so, you'll also need\n",
    "to install a recent version of *larch* (e.g. `conda install \"larch>=5.7.1\" -c conda-forge`).\n",
    "\n",
    "The nesting tree can be defined as usual in Larch, or you can use the\n",
    "`construct_nesting_tree` convenience function to read in a nesting tree\n",
    "definition according to the usual ActivitySim yaml notation, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting_settings = \"\"\"\n",
    "NESTS:\n",
    "  name: root\n",
    "  coefficient: coef_nest_root\n",
    "  alternatives:\n",
    "      - name: MOTORIZED\n",
    "        coefficient: coef_nest_motor\n",
    "        alternatives:\n",
    "            - DRIVE\n",
    "            - TRANSIT\n",
    "      - WALK\n",
    "\"\"\"\n",
    "\n",
    "import yaml\n",
    "\n",
    "from sharrow.nested_logit import construct_nesting_tree\n",
    "\n",
    "nesting_settings = yaml.safe_load(nesting_settings)[\"NESTS\"]\n",
    "nest_tree = construct_nesting_tree(\n",
    "    alternatives=spec.columns[1:], nesting_settings=nesting_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "nest_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Once the nesting tree is defined, it needs to be converted to operating arrays, using the `as_arrays` method (available in larch 5.7.1 and later).  Since we note estimating a nested logit model and just applying one,\n",
    "we can give the parameter values as a dictionary instead of a `larch.Model` to link against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nesting = nest_tree.as_arrays(\n",
    "    trim=True, parameter_dict={\"coef_nest_motor\": 0.5, \"coef_nest_root\": 1.0}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "This dictionary of arrays can be passed in to the `logit_draws` function to compile a nested logit model\n",
    "intead of a simple MNL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time choices_nl, choice_probs_nl = flow.logit_draws(b, draws, nesting=nesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time choices2_nl, choice2_probs_nl = flow.logit_draws(b, draws, source=tree_2, nesting=nesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "choices2_nl_darr, choice2_probs_nl_darr = flow.logit_draws(\n",
    "    b, draws, source=tree_2, nesting=nesting, as_dataarray=True\n",
    ")\n",
    "assert choices2_nl_darr.dims == (\"TOURIDX\",)\n",
    "assert choices2_nl_darr.shape == (100000,)\n",
    "assert choice2_probs_nl_darr.dims == (\"TOURIDX\",)\n",
    "assert choice2_probs_nl_darr.shape == (100000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST devolve NL to MNL\n",
    "choices_nl_1, choice_probs_nl_1 = flow.logit_draws(\n",
    "    b,\n",
    "    draws,\n",
    "    nesting=nest_tree.as_arrays(\n",
    "        trim=True, parameter_dict={\"coef_nest_motor\": 1.0, \"coef_nest_root\": 1.0}\n",
    "    ),\n",
    ")\n",
    "assert (choices_nl_1 == choices).all()\n",
    "assert choice_probs == approx(choice_probs_nl_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "For nested logit models, computing just the logsums is faster than generating probabilities (and making choices) so the `logsums=1` argument allows you to short-circuit the computations if you only want the logsums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.logit_draws(b, draws, source=tree_2, nesting=nesting, logsums=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "_ch, _pr, _pc, _ls = flow.logit_draws(\n",
    "    b, draws, source=tree_2, nesting=nesting, logsums=1\n",
    ")\n",
    "assert _ch is None\n",
    "assert _pr is None\n",
    "assert _pc is None\n",
    "assert _ls.size == 100000\n",
    "np.testing.assert_array_almost_equal(\n",
    "    _ls[:5], [0.532791, 0.490935, 0.557529, 0.556371, 0.54812]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    _ls[-5:], [0.452682, 0.465422, 0.554312, 0.525064, 0.515226]\n",
    ")\n",
    "\n",
    "_ch, _pr, _pc, _ls = flow.logit_draws(\n",
    "    b,\n",
    "    draws,\n",
    "    source=tree_2,\n",
    "    nesting=nesting,\n",
    "    logsums=1,\n",
    "    as_dataarray=True,\n",
    ")\n",
    "assert _ch is None\n",
    "assert _pr is None\n",
    "assert _pc is None\n",
    "assert _ls.size == 100000\n",
    "assert _ls.dims == (\"TOURIDX\",)\n",
    "assert _ls.shape == (100000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST masking\n",
    "masker = np.zeros(draws.shape, dtype=np.int8)\n",
    "masker[::2] = 1\n",
    "_ch_m, _pr_m, _pc_m, _ls_m = flow.logit_draws(\n",
    "    b, draws, source=tree_2, nesting=nesting, logsums=1, mask=masker\n",
    ")\n",
    "\n",
    "assert _ls_m == approx(np.where(masker, _ls, 0))\n",
    "assert (_ch_m, _pr_m, _pc_m) == (None, None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Note that for consistency, the choices, probabilities of choices,\n",
    "and pick count arrays are still returned as the first three elements\n",
    "of the returned tuple, but they're all zero-size empty arrays.\n",
    "\n",
    "To get *both* the logsums and the choices, set `logsums=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.logit_draws(b, draws, source=tree_2, nesting=nesting, logsums=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "_ch, _pr, _pc, _ls = flow.logit_draws(\n",
    "    b, draws, source=tree_2, nesting=nesting, logsums=2\n",
    ")\n",
    "assert _ch.size == 100000\n",
    "assert _pr.size == 100000\n",
    "assert _pc is None\n",
    "assert _ls.size == 100000\n",
    "np.testing.assert_array_almost_equal(_ch[:5], [1, 2, 1, 1, 1])\n",
    "np.testing.assert_array_almost_equal(_ch[-5:], [0, 1, 0, 1, 0])\n",
    "np.testing.assert_array_almost_equal(\n",
    "    _pr[:5], [0.393454, 0.16956, 0.38384, 0.384285, 0.387469]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    _pr[-5:], [0.503606, 0.420874, 0.478898, 0.396506, 0.468742]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    _ls[:5], [0.532791, 0.490935, 0.557529, 0.556371, 0.54812]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    _ls[-5:], [0.452682, 0.465422, 0.554312, 0.525064, 0.515226]\n",
    ")\n",
    "_ch, _pr, _pc, _ls = flow.logit_draws(\n",
    "    b, draws, source=tree_2, nesting=nesting, logsums=2, as_dataarray=True\n",
    ")\n",
    "assert _ch.size == 100000\n",
    "assert _ch.dims == (\"TOURIDX\",)\n",
    "assert _ch.shape == (100000,)\n",
    "assert _pr.size == 100000\n",
    "assert _pr.dims == (\"TOURIDX\",)\n",
    "assert _pr.shape == (100000,)\n",
    "assert _ls.size == 100000\n",
    "assert _ls.dims == (\"TOURIDX\",)\n",
    "assert _ls.shape == (100000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "draws_many = np.random.default_rng(42).random(size=(tree.shape[0], 5))\n",
    "_ch, _pr, _pc, _ls = flow.logit_draws(\n",
    "    b, draws_many, source=tree_2, nesting=nesting, logsums=2, as_dataarray=True\n",
    ")\n",
    "assert _ch.dims == (\"TOURIDX\", \"DRAW\")\n",
    "assert _ch.shape == (100000, 5)\n",
    "assert _pr.dims == (\"TOURIDX\", \"DRAW\")\n",
    "assert _pr.shape == (100000, 5)\n",
    "assert _ls.dims == (\"TOURIDX\",)\n",
    "assert _ls.shape == (100000,)\n",
    "assert _pc is None\n",
    "\n",
    "_ch, _pr, _pc, _ls = flow.logit_draws(\n",
    "    b,\n",
    "    draws_many,\n",
    "    source=tree_2,\n",
    "    nesting=nesting,\n",
    "    logsums=2,\n",
    "    as_dataarray=True,\n",
    "    pick_counted=True,\n",
    ")\n",
    "assert _ch.dims == (\"TOURIDX\", \"DRAW\")\n",
    "assert _ch.shape == (100000, 5)\n",
    "assert _pr.dims == (\"TOURIDX\", \"DRAW\")\n",
    "assert _pr.shape == (100000, 5)\n",
    "assert _ls.dims == (\"TOURIDX\",)\n",
    "assert _ls.shape == (100000,)\n",
    "assert _pc.dims == (\"TOURIDX\", \"DRAW\")\n",
    "assert _pc.shape == (100000, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST masking\n",
    "masker = np.zeros(tree.shape[0], dtype=np.int8)\n",
    "masker[::3] = 1\n",
    "\n",
    "_ch_m, _pr_m, _pc_m, _ls_m = flow.logit_draws(\n",
    "    b,\n",
    "    draws_many,\n",
    "    source=tree_2,\n",
    "    nesting=nesting,\n",
    "    logsums=2,\n",
    "    as_dataarray=True,\n",
    "    mask=masker,\n",
    "    pick_counted=True,\n",
    ")\n",
    "\n",
    "assert (_ch_m.values == (np.where(np.expand_dims(masker, -1), _ch, -1))).all()\n",
    "assert (_pr_m.values == (np.where(np.expand_dims(masker, -1), _pr, 0))).all()\n",
    "assert (_pc_m.values == (np.where(np.expand_dims(masker, -1), _pc, 0))).all()\n",
    "assert (_ls_m.values == (np.where(masker, _ls, 0))).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "## Batch Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "Suppose we want to compute logsums not just for one destination, but for many destinations.  We can construct a `Dataset` with two dimensions to use at the top of our `DataTree`, one for the tours and one for the candidate destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tour_by_dest = tree.subspaces[\"tour\"]\n",
    "tour_by_dest = tour_by_dest.assign_coords(\n",
    "    {\"CAND_DEST\": xr.DataArray(np.arange(25), dims=\"CAND_DEST\")}\n",
    ")\n",
    "tour_by_dest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Then we can create a very similar DataTree as above, using this two dimension root Dataset, but we will point to our destination zones from the new tour dimension. and then create a flow from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_tree = sh.DataTree(\n",
    "    tour=tour_by_dest,\n",
    "    person=persons,\n",
    "    hh=households,\n",
    "    odt_skims=skims,\n",
    "    dot_skims=skims,\n",
    "    relationships=(\n",
    "        \"tour.PERID @ person.PERID\",\n",
    "        \"person.household_id @ hh.HHID\",\n",
    "        \"hh.TAZ @ odt_skims.otaz\",\n",
    "        \"tour.CAND_DEST -> odt_skims.dtaz\",\n",
    "        \"tour.out_time_period @ odt_skims.time_period\",\n",
    "        \"tour.CAND_DEST -> dot_skims.otaz\",\n",
    "        \"hh.TAZ @ dot_skims.dtaz\",\n",
    "        \"tour.in_time_period @ dot_skims.time_period\",\n",
    "    ),\n",
    "    extra_vars={\n",
    "        \"shortwait\": 3,\n",
    "        \"one\": 1,\n",
    "    },\n",
    "    aux_vars={\n",
    "        \"short_i_wait_mult\": 0.75,\n",
    "        \"income_breakpoints\": income_breakpoints,\n",
    "    },\n",
    "    dim_order=(\"TOURIDX\", \"CAND_DEST\"),\n",
    ")\n",
    "wide_flow = wide_tree.setup_flow(spec.Expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_logsums = wide_flow.logit_draws(b, logsums=1, compile_watch=\"simple\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wide_logsums = wide_flow.logit_draws(b, logsums=1, compile_watch=\"simple\")[-1]\n",
    "wide_logsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "np.testing.assert_array_almost_equal(\n",
    "    wide_logsums[:5, :5],\n",
    "    np.array(\n",
    "        [\n",
    "            [0.759222, 0.75862, 0.744936, 0.758251, 0.737007],\n",
    "            [0.671698, 0.671504, 0.663015, 0.661482, 0.667133],\n",
    "            [0.670188, 0.678498, 0.687647, 0.691152, 0.715783],\n",
    "            [0.760743, 0.769123, 0.763733, 0.784487, 0.802356],\n",
    "            [0.73474, 0.743051, 0.751439, 0.754731, 0.778121],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    ),\n",
    ")\n",
    "np.testing.assert_array_almost_equal(\n",
    "    wide_logsums[-5:, -5:],\n",
    "    np.array(\n",
    "        [\n",
    "            [0.719523, 0.755152, 0.739368, 0.762664, 0.764388],\n",
    "            [0.740303, 0.678783, 0.649964, 0.694407, 0.681555],\n",
    "            [0.758865, 0.663663, 0.637266, 0.673351, 0.65875],\n",
    "            [0.765125, 0.706478, 0.676878, 0.717814, 0.713912],\n",
    "            [0.73348, 0.683626, 0.647698, 0.69146, 0.673006],\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "np.testing.assert_array_almost_equal(\n",
    "    wide_logsums[np.arange(len(tours)), tours[\"dest_taz_idx\"].to_numpy()],\n",
    "    flow.logit_draws(b, logsums=1)[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "wide_logsums_ = wide_flow.logit_draws(\n",
    "    b, logsums=1, compile_watch=True, as_dataarray=True\n",
    ")[-1]\n",
    "assert wide_logsums_.dims == (\"TOURIDX\", \"CAND_DEST\")\n",
    "assert wide_logsums_.shape == (100000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "wide_draws = np.random.default_rng(42).random(size=wide_tree.shape)\n",
    "with pytest.warns(sh.CacheMissWarning):\n",
    "    wide_logsums_plus = wide_flow.logit_draws(\n",
    "        b, logsums=2, compile_watch=True, as_dataarray=True, draws=wide_draws\n",
    "    )\n",
    "assert wide_logsums_plus[0].dims == (\"TOURIDX\", \"CAND_DEST\")\n",
    "assert wide_logsums_plus[0].shape == (100000, 25)\n",
    "assert wide_logsums_plus[3].dims == (\"TOURIDX\", \"CAND_DEST\")\n",
    "assert wide_logsums_plus[3].shape == (100000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "wide_draws = np.random.default_rng(42).random(size=wide_tree.shape + (2,))\n",
    "wide_logsums_plus = wide_flow.logit_draws(\n",
    "    b, logsums=2, compile_watch=True, as_dataarray=True, draws=wide_draws\n",
    ")\n",
    "assert wide_logsums_plus[0].dims == (\"TOURIDX\", \"CAND_DEST\", \"DRAW\")\n",
    "assert wide_logsums_plus[0].shape == (100000, 25, 2)\n",
    "assert wide_logsums_plus[3].dims == (\"TOURIDX\", \"CAND_DEST\")\n",
    "assert wide_logsums_plus[3].shape == (100000, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST masking\n",
    "mask = np.zeros(wide_tree.shape, dtype=np.int8)\n",
    "mask[::7] = 1\n",
    "with pytest.warns(sh.CacheMissWarning):\n",
    "    wide_logsums_mask = wide_flow.logit_draws(\n",
    "        b, logsums=2, compile_watch=True, as_dataarray=True, draws=wide_draws, mask=mask\n",
    "    )\n",
    "assert wide_logsums_mask[0].dims == (\"TOURIDX\", \"CAND_DEST\", \"DRAW\")\n",
    "assert wide_logsums_mask[0].shape == (100000, 25, 2)\n",
    "assert wide_logsums_mask[3].dims == (\"TOURIDX\", \"CAND_DEST\")\n",
    "assert wide_logsums_mask[3].shape == (100000, 25)\n",
    "\n",
    "assert (\n",
    "    wide_logsums_plus[0].where(np.expand_dims(mask, -1), -1) == wide_logsums_mask[0]\n",
    ").all()\n",
    "assert (\n",
    "    wide_logsums_plus[1].where(np.expand_dims(mask, -1), 0) == wide_logsums_mask[1]\n",
    ").all()\n",
    "assert (wide_logsums_plus[3].where(mask, 0) == wide_logsums_mask[3]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST masking performance\n",
    "import timeit\n",
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"error\")\n",
    "    masked_time = timeit.timeit(\n",
    "        lambda: wide_flow.logit_draws(\n",
    "            b,\n",
    "            logsums=2,\n",
    "            compile_watch=True,\n",
    "            as_dataarray=True,\n",
    "            draws=wide_draws,\n",
    "            mask=mask,\n",
    "        ),\n",
    "        number=1,\n",
    "    )\n",
    "    raw_time = timeit.timeit(\n",
    "        lambda: wide_flow.logit_draws(\n",
    "            b, logsums=2, compile_watch=True, as_dataarray=True, draws=wide_draws\n",
    "        ),\n",
    "        number=1,\n",
    "    )\n",
    "assert masked_time < raw_time  # generous, should be nearly 7 times faster\n",
    "assert len(wide_flow.cache_misses[\"_imnl_plus1d\"]) == 3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
