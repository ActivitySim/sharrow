{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multi-Dimensional Analysis\n",
    "\n",
    "This notebook provides a walkthrough of some of the multi-dimensional analysis\n",
    "capabilities of the `sharrow` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import sharrow as sh\n",
    "\n",
    "sh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST check versions\n",
    "import packaging\n",
    "\n",
    "assert packaging.version.parse(xr.__version__) >= packaging.version.parse(\"0.20.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "\n",
    "We'll begin by again importing some example data to work with.  We'll be using \n",
    "some test data taken from the MTC example in the ActivitySim project, including \n",
    "tables of data for households and persons, as well as a set of \n",
    "skims containing transportation level of service information for travel around\n",
    "a tiny slice of San Francisco.\n",
    "\n",
    "The households and persons are typical tabular data, and \n",
    "each can be read in and stored as a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "households = sh.example_data.get_households()\n",
    "households.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test households content\n",
    "assert len(households) == 5000\n",
    "assert \"income\" in households\n",
    "assert households.index.name == \"HHID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = sh.example_data.get_persons()\n",
    "persons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert len(persons) == 8212\n",
    "assert \"household_id\" in persons\n",
    "assert persons.index.name == \"PERID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The skims, on the other hand, are not just simple tabular data, but rather a \n",
    "multi-dimensional representation of the transportation system, indexed by origin.\n",
    "destination, and time of day. Rather than using a single DataFrame for this data,\n",
    "we store it as a multi-dimensional `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims = sh.example_data.get_skims()\n",
    "skims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "For this example, we'll also load a land use table, that contains some attributes of the alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse = sh.example_data.get_land_use()\n",
    "landuse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Multi-Dimensional Analysis\n",
    "\n",
    "Now that we're loaded our inputs, let's take a look at \n",
    "preparing some data for a workplace location choice simulation model.\n",
    "This is a different kind of model, and it will use differently shaped data: the decision \n",
    "makers (or \"choosers\") in this model will be the workers, and the alternatives \n",
    "will be the various zones included in the land use table.\n",
    "\n",
    "The workers are only a subset of the persons data we looked at before.  We can identify workers from\n",
    "values 1 and 2 (full-time employed and part-time employed) in the `'pemploy'` attribute \n",
    "of the `persons` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = persons.query(\"pemploy in [1,2]\").rename_axis(index=\"WORKERID\")\n",
    "workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "As we filter the persons table to just the workers, we also renamed the index from\n",
    "\"PERSONID\" to \"WORKERID\".  This renaming is important for `sharrow`, as it expects dimensions\n",
    "that have the same name to match, but the workers don't align directly with the persons \n",
    "anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "For our workplace location choice model, we will want to link in data from our skims,\n",
    "which can tell us about travel times and costs.  Since we have not yet determined a \n",
    "time of day for each worker's work tours, we'll just use the `'AM'` skims for the outbound\n",
    "leg of a hypothetical work tour, and the `'PM'` skims for the return leg.  Instead of \n",
    "trying to select constant skims using the dynamic lookups that sharrow can compile, \n",
    "we can just filter the skims down in a static manner before placing them into the data tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_am = skims.sel(time_period=\"AM\")\n",
    "skims_pm = skims.sel(time_period=\"PM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Creating a DataTree Iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The last step in getting ready for this model is building out the relationships between all\n",
    "this data we've prepared. We'll again use the `DataTree` class to do that, but this time \n",
    "we'll demostrate building the tree in stages.  First, we'll create a\n",
    "base Dataset to be the root data for the tree. We can start by creating an otherwise empty `Dataset` indexed on the two dimensions we want to end up with for analysis, workers and zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = sh.dataset.from_named_objects(\n",
    "    workers.index,\n",
    "    landuse.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Since our base dataset has\n",
    "two dimensions, we can specify a dimension order when writing into\n",
    "a DataTree (the default is alphabetical order).\n",
    "This ordering will be applied to outputs from the flows later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = sh.DataTree(base=base, dim_order=(\"WORKERID\", \"TAZ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST tree_dest attributes\n",
    "assert tree.dim_order == (\"WORKERID\", \"TAZ\")\n",
    "assert tree.shape == (4361, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Then, we can progressively build our `DataTree` by adding additional data. \n",
    "Each new branch of the tree we want to add using the `add_dataset` command should have a \n",
    "unique name, a dataset being attached, and one or more relationship declarations\n",
    "that describe how the new data attaches.  For example, we can attach the `persons`\n",
    "data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add_dataset(\"person\", persons, \"base.WORKERID @ person.PERID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "The relationship definition here starts with a dotted name of some data \n",
    "dimension already in the tree, an `@` operator to indicating matching by\n",
    "label in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add_dataset(\"landuse\", landuse, \"base.TAZ @ landuse.TAZ\")\n",
    "tree.add_dataset(\"hh\", households, \"person.household_id @ hh.HHID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Unlike in the mode choice work in the previous example, we've already filtered the \n",
    "time period dimensions of the skims to be morning and afternoon peak,\n",
    "so we simply attach the two different parts, linking relationships only\n",
    "for the remaining dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.add_dataset(\n",
    "    \"odskims\",\n",
    "    skims_am,\n",
    "    relationships=(\n",
    "        \"hh.TAZ @ odskims.otaz\",\n",
    "        \"base.TAZ @ odskims.dtaz\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "tree.add_dataset(\n",
    "    \"doskims\",\n",
    "    skims_pm,\n",
    "    relationships=(\n",
    "        \"base.TAZ @ doskims.otaz\",\n",
    "        \"hh.TAZ @ doskims.dtaz\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Dynamically Defined Flows \n",
    "\n",
    "Although it is convenient to write expressions into a seperately configured \n",
    "\"spec\" file, especially when working with \n",
    "ActivitySim, it's not strictly necessary to employ such a file in csv format; \n",
    "a simple Python dictionary can also be used to setup a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = {\n",
    "    \"round_trip_dist\": \"odskims.DIST + doskims.DIST\",\n",
    "    \"round_trip_dist_first_mile\": \"clip(odskims.DIST, 0, 1) + clip(doskims.DIST, 0, 1)\",\n",
    "    \"round_trip_dist_addl_miles\": \"clip(odskims.DIST-1, 0, None) + clip(doskims.DIST-1, 0, None)\",\n",
    "    \"size_term\": \"log(TOTPOP + 0.5*EMPRES)\",\n",
    "}\n",
    "\n",
    "flow = tree.setup_flow(definition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Loading from this flow is done the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = flow.load()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert arr.shape == (4361, 25, 4)\n",
    "expected = np.array(\n",
    "    [\n",
    "        [\n",
    "            [0.61, 0.61, 0.0, 4.610157],\n",
    "            [0.28, 0.28, 0.0, 5.681878],\n",
    "            [0.56, 0.56, 0.0, 6.368187],\n",
    "            [0.53, 0.53, 0.0, 5.741399],\n",
    "            [1.23, 1.23, 0.0, 7.17549],\n",
    "        ],\n",
    "        [\n",
    "            [1.19, 1.19, 0.0, 4.610157],\n",
    "            [1.49, 1.49, 0.0, 5.681878],\n",
    "            [1.88, 1.85, 0.03, 6.368187],\n",
    "            [1.36, 1.36, 0.0, 5.741399],\n",
    "            [1.93, 1.93, 0.0, 7.17549],\n",
    "        ],\n",
    "        [\n",
    "            [1.19, 1.19, 0.0, 4.610157],\n",
    "            [1.49, 1.49, 0.0, 5.681878],\n",
    "            [1.88, 1.85, 0.03, 6.368187],\n",
    "            [1.36, 1.36, 0.0, 5.741399],\n",
    "            [1.93, 1.93, 0.0, 7.17549],\n",
    "        ],\n",
    "        [\n",
    "            [0.24, 0.24, 0.0, 4.610157],\n",
    "            [0.61, 0.61, 0.0, 5.681878],\n",
    "            [1.01, 1.01, 0.0, 6.368187],\n",
    "            [0.75, 0.75, 0.0, 5.741399],\n",
    "            [1.38, 1.38, 0.0, 7.17549],\n",
    "        ],\n",
    "        [\n",
    "            [0.61, 0.61, 0.0, 4.610157],\n",
    "            [0.28, 0.28, 0.0, 5.681878],\n",
    "            [0.56, 0.56, 0.0, 6.368187],\n",
    "            [0.53, 0.53, 0.0, 5.741399],\n",
    "            [1.23, 1.23, 0.0, 7.17549],\n",
    "        ],\n",
    "    ],\n",
    "    dtype=np.float32,\n",
    ")\n",
    "\n",
    "np.testing.assert_array_almost_equal(arr[:5, :5, :], expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "For the tour mode example above, the tours dataset had only one dimension (TOURIDX),\n",
    "and so the output of the load function had two dimensions (TOURIDX and expressions).\n",
    "In this example, the base dataset in the tree has two dimensions (workers and zones)\n",
    "and so the result from the basic `load` function has *three* dimensions (workers, zones, and expressions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "Just as we could neatly format the two-dimensional output above as a `pandas.DataFrame`,\n",
    "so too can we neatly format this three-dimensional output, as a `xarray.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pretty = flow.load_dataarray()\n",
    "arr_pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert isinstance(arr_pretty, xr.DataArray)\n",
    "assert arr_pretty.dims == (\"WORKERID\", \"TAZ\", \"expressions\")\n",
    "assert arr_pretty.shape == (4361, 25, 4)\n",
    "assert all(\n",
    "    arr_pretty.expressions\n",
    "    == np.array(\n",
    "        [\n",
    "            \"round_trip_dist\",\n",
    "            \"round_trip_dist_first_mile\",\n",
    "            \"round_trip_dist_addl_miles\",\n",
    "            \"size_term\",\n",
    "        ],\n",
    "        dtype=\"<U26\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Linear-in-Parameters Functions\n",
    "\n",
    "We can also use the `dot` method here with the two dimensional base.\n",
    "We'll apply a one-dimensional coefficients array, with length three to \n",
    "match the three terms in the spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.asarray([1.0, 0.1, 0.01, 0.001])\n",
    "flow.dot(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The `dot_dataarray` method does the same underlying computational work, but \n",
    "yields a well-formatted DataArray intead of just a plain numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow.dot_dataarray(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "## Multinomial Logit Simulation\n",
    "\n",
    "And we can build and simulate an MNL model directly using the `logit_draws` method.  \n",
    "To do so we need to\n",
    "provide the \"random\" draws exogenously.  Here, we'll sample 10 zones (with\n",
    "replacement) from the selection of alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws = np.random.default_rng(123).random(size=[4361, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices, choice_probs = flow.logit_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "choice_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "expected_ch = np.array(\n",
    "    [\n",
    "        [5, 8, 8, 9, 9, 18, 19, 19, 19, 20],\n",
    "        [7, 7, 7, 9, 10, 17, 18, 19, 20, 23],\n",
    "        [1, 6, 6, 7, 9, 9, 13, 17, 18, 22],\n",
    "        [8, 9, 9, 18, 18, 19, 19, 19, 19, 20],\n",
    "        [2, 4, 6, 8, 9, 10, 17, 17, 18, 18],\n",
    "        # ...,\n",
    "        [0, 1, 7, 9, 13, 21, 22, 22, 24, 24],\n",
    "        [0, 5, 5, 6, 8, 8, 18, 21, 22, 22],\n",
    "        [5, 6, 7, 13, 15, 22, 22, 22, 23, 23],\n",
    "        [0, 2, 3, 13, 16, 22, 22, 22, 24, 24],\n",
    "        [7, 8, 8, 11, 14, 14, 16, 21, 22, 22],\n",
    "    ],\n",
    "    dtype=np.int32,\n",
    ")\n",
    "np.testing.assert_array_equal(choices[:5], expected_ch[:5])\n",
    "np.testing.assert_array_equal(choices[-5:], expected_ch[-5:])\n",
    "\n",
    "expected_pr = np.array(\n",
    "    [\n",
    "        [\n",
    "            0.021757,\n",
    "            0.082092,\n",
    "            0.082092,\n",
    "            0.090812,\n",
    "            0.090812,\n",
    "            0.239048,\n",
    "            0.130507,\n",
    "            0.130507,\n",
    "            0.130507,\n",
    "            0.038469,\n",
    "        ],\n",
    "        [\n",
    "            0.063636,\n",
    "            0.063636,\n",
    "            0.063636,\n",
    "            0.103338,\n",
    "            0.039564,\n",
    "            0.035372,\n",
    "            0.10316,\n",
    "            0.064873,\n",
    "            0.021167,\n",
    "            0.031342,\n",
    "        ],\n",
    "        [\n",
    "            0.017309,\n",
    "            0.052503,\n",
    "            0.052503,\n",
    "            0.063636,\n",
    "            0.103338,\n",
    "            0.103338,\n",
    "            0.008113,\n",
    "            0.035372,\n",
    "            0.10316,\n",
    "            0.054565,\n",
    "        ],\n",
    "        [\n",
    "            0.08459,\n",
    "            0.094525,\n",
    "            0.094525,\n",
    "            0.246322,\n",
    "            0.246322,\n",
    "            0.134478,\n",
    "            0.134478,\n",
    "            0.134478,\n",
    "            0.134478,\n",
    "            0.040041,\n",
    "        ],\n",
    "        [\n",
    "            0.006765,\n",
    "            0.014148,\n",
    "            0.027726,\n",
    "            0.082092,\n",
    "            0.090812,\n",
    "            0.035121,\n",
    "            0.082798,\n",
    "            0.082798,\n",
    "            0.239048,\n",
    "            0.239048,\n",
    "        ],\n",
    "        # ...,\n",
    "        [\n",
    "            0.046512,\n",
    "            0.039614,\n",
    "            0.019715,\n",
    "            0.028343,\n",
    "            0.031909,\n",
    "            0.08728,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "            0.06648,\n",
    "            0.06648,\n",
    "        ],\n",
    "        [\n",
    "            0.046512,\n",
    "            0.039726,\n",
    "            0.039726,\n",
    "            0.027111,\n",
    "            0.038968,\n",
    "            0.038968,\n",
    "            0.028924,\n",
    "            0.08728,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "        ],\n",
    "        [\n",
    "            0.039726,\n",
    "            0.027111,\n",
    "            0.019715,\n",
    "            0.031909,\n",
    "            0.023773,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "            0.069134,\n",
    "            0.069134,\n",
    "        ],\n",
    "        [\n",
    "            0.046512,\n",
    "            0.036197,\n",
    "            0.025022,\n",
    "            0.031909,\n",
    "            0.03535,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "            0.06648,\n",
    "            0.06648,\n",
    "        ],\n",
    "        [\n",
    "            0.019715,\n",
    "            0.038968,\n",
    "            0.038968,\n",
    "            0.013389,\n",
    "            0.048031,\n",
    "            0.048031,\n",
    "            0.03535,\n",
    "            0.08728,\n",
    "            0.207882,\n",
    "            0.207882,\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(choice_probs[:5], expected_pr[:5])\n",
    "np.testing.assert_array_almost_equal(choice_probs[-5:], expected_pr[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "choices_darr, choice_probs_darr = flow.logit_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws,\n",
    "    as_dataarray=True,\n",
    ")\n",
    "assert choices_darr.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert choices_darr.shape == (4361, 10)\n",
    "np.testing.assert_array_equal(choices_darr[:5], expected_ch[:5])\n",
    "np.testing.assert_array_equal(choices_darr[-5:], expected_ch[-5:])\n",
    "assert choice_probs_darr.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert choice_probs_darr.shape == (4361, 10)\n",
    "np.testing.assert_array_almost_equal(choice_probs_darr[:5], expected_pr[:5])\n",
    "np.testing.assert_array_almost_equal(choice_probs_darr[-5:], expected_pr[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "It's more common to make many repeated choices for destination choice type models\n",
    "(e.g. to sample destinations), so there's also a \"pick count\" feature, that\n",
    "can summarize the simulation results efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_, choice_probs_, pick_count = flow.logit_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws,\n",
    "    pick_counted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "If you compare against the non-pick-counted results above, you'll see \n",
    "that we get exactly the same choices out, but when choices are repeated\n",
    "they are aggregated in the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "pick_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST pick count results\n",
    "for i in range(choices.shape[0]):\n",
    "    uc, pc = np.unique(choices[i], return_counts=True)\n",
    "    np.testing.assert_array_equal(uc, choices_[i, : len(uc)])\n",
    "    np.testing.assert_array_equal(np.full(10 - len(uc), -1), choices_[i, len(uc) :])\n",
    "    np.testing.assert_array_equal(pc, pick_count[i, : len(uc)])\n",
    "    np.testing.assert_array_equal(np.zeros(10 - len(uc)), pick_count[i, len(uc) :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "choices__darr, choice_probs__darr, pick_count_darr = flow.logit_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws,\n",
    "    pick_counted=True,\n",
    "    as_dataarray=True,\n",
    ")\n",
    "assert choices__darr.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert choices__darr.shape == (4361, 10)\n",
    "assert choice_probs__darr.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert choice_probs__darr.shape == (4361, 10)\n",
    "assert pick_count_darr.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert pick_count_darr.shape == (4361, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### Accessing Logsums\n",
    "\n",
    "If you want to also access the MNL logsum values from the choice model, \n",
    "adding `logsums=True` will return those values in the fourth position of \n",
    "the returned tuple (even if pick counting is disabled, the logsum array\n",
    "is in the 4th value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices, choice_probs, _, logsums = flow.logit_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws,\n",
    "    logsums=True,\n",
    ")\n",
    "logsums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST logsums\n",
    "expected = np.array(\n",
    "    [\n",
    "        5.618349,\n",
    "        5.701237,\n",
    "        5.701237,\n",
    "        5.739875,\n",
    "        5.618349,\n",
    "        # ...,\n",
    "        5.585756,\n",
    "        5.585756,\n",
    "        5.585756,\n",
    "        5.585756,\n",
    "        5.585756,\n",
    "    ]\n",
    ")\n",
    "np.testing.assert_array_almost_equal(logsums[:5], expected[:5])\n",
    "np.testing.assert_array_almost_equal(logsums[-5:], expected[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "choices_darr2, choice_probs_darr2, pick_count_nope, logsums_darr = flow.logit_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws,\n",
    "    logsums=True,\n",
    "    as_dataarray=True,\n",
    ")\n",
    "assert choices_darr2.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert choices_darr2.shape == (4361, 10)\n",
    "assert choice_probs_darr2.dims == (\"WORKERID\", \"DRAW\")\n",
    "assert choice_probs_darr2.shape == (4361, 10)\n",
    "assert pick_count_nope is None\n",
    "assert logsums_darr.dims == (\"WORKERID\",)\n",
    "assert logsums_darr.shape == (4361,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "## Gotchas\n",
    "\n",
    "When working with multi-dimension outputs, if you don't specify the dimensions ordering\n",
    "explicitly (as done above) then the output dimensions will be in lexicographic order \n",
    "according to the unicode binary representations of the dimension names.  This is similar\n",
    "to alphabetical ordering, except all uppercase letters come before lower case letters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_unordered = sh.DataTree(\n",
    "    base=base,\n",
    "    person=persons,\n",
    "    landuse=landuse,\n",
    "    hh=households,\n",
    "    odskims=skims_am,\n",
    "    doskims=skims_pm,\n",
    "    relationships=(\n",
    "        \"base.WORKERID @ person.PERID\",\n",
    "        \"base.TAZ @ landuse.TAZ\",\n",
    "        \"person.household_id @ hh.HHID\",\n",
    "        \"hh.TAZ @ odskims.otaz\",\n",
    "        \"base.TAZ @ odskims.dtaz\",\n",
    "        \"base.TAZ @ doskims.otaz\",\n",
    "        \"hh.TAZ @ doskims.dtaz\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST tree_unordered attributes\n",
    "assert tree_unordered.dim_order is None\n",
    "assert tree_unordered.shape == (25, 4361)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_unordered = tree_unordered.setup_flow(definition)\n",
    "arr_unordered = flow_unordered.load_dataarray()\n",
    "arr_unordered.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST flow_unordered\n",
    "assert arr_unordered.dims == (\"TAZ\", \"WORKERID\", \"expressions\")\n",
    "assert arr_unordered.shape == (25, 4361, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
