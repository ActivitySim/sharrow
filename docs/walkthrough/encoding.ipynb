{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Data Encoding\n",
    "\n",
    "This notebook provides a short walkthrough of some of the data encoding features of the `sharrow` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# HIDDEN\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import sharrow as sh\n",
    "\n",
    "sh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# check versions\n",
    "import packaging\n",
    "\n",
    "assert packaging.version.parse(xr.__version__) >= packaging.version.parse(\"0.20.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "import pytest\n",
    "\n",
    "pytest.importorskip(\"sparse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "\n",
    "We'll begin by importing some example data to work with.  We'll be using \n",
    "some test data taken from the MTC example in the ActivitySim project. For\n",
    "this data encoding walkthrough, we'll focus on the\n",
    "skims containing transportation level of service information for travel around\n",
    "a tiny slice of San Francisco."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "We'll load them as a multi-dimensional `xarray.Dataset` â€” or, more exactly, a \n",
    "`sharrow.Dataset`, which is a subclass from the xarray version that adds some \n",
    "useful features, including compatability with automatic tools for recoding data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims = sh.example_data.get_skims()\n",
    "skims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Because sharrow uses the `xarray.Dataset` format to work with data, individual \n",
    "variables in each Dataset can be encoded in different data types.\n",
    "For example, automobile travel times can be stored with \n",
    "high(er) precision floating point numbers, while transit \n",
    "fares, which vary less and have a narrower range, can be \n",
    "stored with lower precision.  This allows a user to choose \n",
    "the most efficient encoding for each variable, if desired. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Fixed Point Encoding\n",
    "\n",
    "Very often, data (especially skim matrixes like here) can be expressed adequately \n",
    "with far less precicion than a standard 32-bit floating point representation allows.\n",
    "In these cases, it may be beneficial to store this \n",
    "data with \"fixed point\" encoding, which is also \n",
    "sometimes called scaled integers.\n",
    "\n",
    "Instead of storing values as 32-bit floating point values, \n",
    "they could be multiplied by a scale factor (e.g., 100) \n",
    "and then converted to 16-bit integers. This uses half the\n",
    "RAM and can still express any value (to two decimal point \n",
    "precision) up to positive or negative 327.68.  If the lowest \n",
    "values in that range are never needed, it can also be shifted,\n",
    "moving both the bottom and top limits by a fixed amount. Then, \n",
    "for a particular scale $\\mu$ and shift $\\xi$ (stored in metadata),\n",
    "from any array element $i$ the implied (original) value $x$ \n",
    "can quickly be recovered by evaluating $(i / \\mu) - \\xi$.\n",
    "\n",
    "Sharrow includes a pair of functions to encode and decode arrays in\n",
    "this manner. These functions also attach the necessary metadata\n",
    "to the Dataset objects, so that later when we construct `sharrow.Flow` \n",
    "instances, they can decode arrays automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharrow.digital_encoding import array_decode, array_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The distance data in the skims is a great candidate for fixed point\n",
    "of encoding.  We can peek at the top corner of this array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims.DIST.values[:2, :3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The data are all small(ish) values with two decimal point fixed\n",
    "precision, so we can probably efficiently encode this data by scaling by 100.\n",
    "If we're not sure, we can confirm by checking the range of values, to make\n",
    "sure it fits inside the 16-bit integers we're hoping to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims.DIST.values.min(), skims.DIST.values.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "That's a really small range because this is only test data.  But even \n",
    "the full-scale MTC skims spanning the entire region don't contain distances\n",
    "over 300 miles.\n",
    "\n",
    "We can create a new DataArray and apply fixed point encoding using the\n",
    "`array_encode` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_encoded = array_encode(skims.DIST, scale=0.01, offset=0)\n",
    "distance_encoded.values[:2, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST encoding\n",
    "assert distance_encoded.dtype == np.int16\n",
    "np.testing.assert_array_equal(\n",
    "    distance_encoded.values[:2, :3],\n",
    "    np.array([[12, 24, 44], [37, 14, 28]], dtype=np.int16),\n",
    ")\n",
    "assert distance_encoded.attrs[\"digital_encoding\"] == {\n",
    "    \"scale\": 0.01,\n",
    "    \"offset\": 0,\n",
    "    \"missing_value\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We can apply that function for any number of variables in the skims, and\n",
    "create a new Dataset that includes the encoded arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_encoded = skims.assign({\"DIST\": array_encode(skims.DIST, scale=0.01, offset=0)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "To manage the digital encodings across an entire dataset, sharrow implements\n",
    "a `digital_encoding` accessor.  You can use it to apply encodings to one or more\n",
    "variables in a simple fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_encoded = skims_encoded.digital_encoding.set(\n",
    "    [\"DISTWALK\", \"DISTBIKE\"], scale=0.01, offset=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "And you can review the encodings for every variable in the dataset like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_encoded.digital_encoding.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert skims_encoded.digital_encoding.info() == {\n",
    "    \"DIST\": {\"scale\": 0.01, \"offset\": 0, \"missing_value\": None},\n",
    "    \"DISTBIKE\": {\"scale\": 0.01, \"offset\": 0, \"missing_value\": None},\n",
    "    \"DISTWALK\": {\"scale\": 0.01, \"offset\": 0, \"missing_value\": None},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "To demonstrate that the encoding works transparently with a `Flow`,\n",
    "we can construct a simple flow that extracts the distance and \n",
    "square of distance for the top corner of values we looked at above.\n",
    "\n",
    "First we'll do so for a flow with the original float32 encoded skims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = pd.DataFrame({\"orig\": [0, 0, 0, 1, 1, 1], \"dest\": [0, 1, 2, 0, 1, 2]})\n",
    "tree = sh.DataTree(\n",
    "    base=pairs,\n",
    "    skims=skims.drop_dims(\"time_period\"),\n",
    "    relationships=(\n",
    "        \"base.orig -> skims.otaz\",\n",
    "        \"base.dest -> skims.dtaz\",\n",
    "    ),\n",
    ")\n",
    "flow = tree.setup_flow({\"d1\": \"DIST\", \"d2\": \"DIST**2\"})\n",
    "arr = flow.load()\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "We can do the same for the encoded skims, and we get exactly the\n",
    "same result, even though the encoded skims use less RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_enc = sh.DataTree(\n",
    "    base=pairs,\n",
    "    skims=skims_encoded.drop_dims(\"time_period\"),\n",
    "    relationships=(\n",
    "        \"base.orig -> skims.otaz\",\n",
    "        \"base.dest -> skims.dtaz\",\n",
    "    ),\n",
    ")\n",
    "flow_enc = tree_enc.setup_flow({\"d1\": \"DIST\", \"d2\": \"DIST**2\"})\n",
    "arr_enc = flow_enc.load()\n",
    "arr_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "np.testing.assert_array_almost_equal(arr, arr_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Dictionary Encoding\n",
    "\n",
    "For skim matrixes where the universe of all possible \n",
    "cell values can be adequately represented by just 255 \n",
    "unique values, we can use an explicit mapping process\n",
    "called \"dictionary encoding\", which works by storing \n",
    "those unique values in a tiny base array.  Then, in the \n",
    "main body of the skim data we only store offsets that point to \n",
    "positions in that base array. This reduces the marginal \n",
    "memory footprint of each array cell to just an 8 bit \n",
    "integer, reducing memory requirements by up to 75% for \n",
    "these arrays compared to float32's. This approach is \n",
    "particularly appropriate for many transit skims, as fares, \n",
    "wait times, and transfers can almost always be reduced \n",
    "to a dictionary encoding with no meaningful information \n",
    "loss.\n",
    "\n",
    "For example, the `'WLK_LOC_WLK_FAR'` array containing fares\n",
    "only has four unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(skims.WLK_LOC_WLK_FAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "We can see various fares applied at different time periods if we\n",
    "look at the top corner of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims.WLK_LOC_WLK_FAR.values[:2, :3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Once encoded, the array itself only contains offset pointers (small integers),\n",
    "plus the original values stored in metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlwfare_enc = array_encode(skims.WLK_LOC_WLK_FAR, bitwidth=8, by_dict=True)\n",
    "wlwfare_enc.values[:2, :3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlwfare_enc.attrs[\"digital_encoding\"][\"dictionary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST encoding\n",
    "assert wlwfare_enc.dtype == np.uint8\n",
    "np.testing.assert_array_equal(\n",
    "    wlwfare_enc.values[:2, :3, :],\n",
    "    np.array(\n",
    "        [\n",
    "            [[0, 0, 0, 0, 0], [1, 2, 2, 1, 2], [1, 2, 2, 1, 2]],\n",
    "            [[1, 1, 2, 2, 1], [0, 0, 0, 0, 0], [1, 2, 2, 1, 2]],\n",
    "        ],\n",
    "        dtype=np.uint8,\n",
    "    ),\n",
    ")\n",
    "np.testing.assert_array_equal(\n",
    "    wlwfare_enc.attrs[\"digital_encoding\"][\"dictionary\"],\n",
    "    np.array([0.0, 152.0, 474.0, 626.0], dtype=np.float32),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "If we want to recover the original data for analysis (other than in\n",
    "a Flow, which can decode it automatically), we can use the `array_decode` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_decode(wlwfare_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TEST\n",
    "xr.testing.assert_equal(array_decode(wlwfare_enc), skims.WLK_LOC_WLK_FAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## Joint Dict Encoding\n",
    "\n",
    "Dictionary encoding can be expanded to map multiple different variables\n",
    "using the same underlying offsets array.  For large datasets with several\n",
    "dimension lengths in the thousands, the offset array may constitute the \n",
    "vast majority of the memory usage, so sharing the same offsets for several\n",
    "variables can result in huge reductions in the memory footprint.\n",
    "\n",
    "The joint dictionary can be applied using the `set` method of the\n",
    "`digital_encoding` accessor, giving a list of the variables to jointly encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims1 = skims.digital_encoding.set(\n",
    "    [\n",
    "        \"WLK_LOC_WLK_FAR\",\n",
    "        \"WLK_EXP_WLK_FAR\",\n",
    "        \"WLK_HVY_WLK_FAR\",\n",
    "        \"DRV_LOC_WLK_FAR\",\n",
    "        \"DRV_HVY_WLK_FAR\",\n",
    "        \"DRV_EXP_WLK_FAR\",\n",
    "    ],\n",
    "    joint_dict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "A unique name is automatically generated for the join when `joint_dict` is set\n",
    "to `True`.  Alternatively, the user can specify a name to use for the join by\n",
    "giving a string input as the `joint_dict`.  Different sets of variables in the \n",
    "same Dataset can be grouped and encoded jointly, but each group must have a \n",
    "unique name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims1 = skims1.digital_encoding.set(\n",
    "    [\"DISTBIKE\", \"DISTWALK\"],\n",
    "    joint_dict=\"jointWB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "The resulting dataset adds a variable for each created group, which contains \n",
    "the offsets, and the named variables in the group are replaced with a new\n",
    "one-dimension array with coordinating lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "Skims encoded in this manner can be fed into sharrow and will compile and return \n",
    "the same results as if they were not encoded.  If you are mixing compiled flows\n",
    "between encoded and unencoded Datasets (which should be unusual, but for the examples\n",
    "in this notebook we've done it) you'll need to set the `hashing_level` to at least\n",
    "2, to ensure you are matching the correct numba code with the encodings used in the\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree1 = sh.DataTree(\n",
    "    base=pairs,\n",
    "    skims=skims1,\n",
    "    rskims=skims1,\n",
    "    relationships=(\n",
    "        \"base.orig -> skims.otaz\",\n",
    "        \"base.dest -> skims.dtaz\",\n",
    "        \"base.orig -> rskims.dtaz\",\n",
    "        \"base.dest -> rskims.otaz\",\n",
    "    ),\n",
    ")\n",
    "flow1 = tree1.setup_flow(\n",
    "    {\n",
    "        \"d1\": 'skims[\"WLK_LOC_WLK_FAR\", \"AM\"]',\n",
    "        \"d2\": 'skims[\"WLK_LOC_WLK_FAR\", \"AM\"]**2',\n",
    "        \"w1\": \"skims.DISTWALK\",\n",
    "        \"w2\": 'skims.reverse(\"DISTWALK\")',\n",
    "        \"w3\": \"rskims.DISTWALK\",\n",
    "        \"x1\": \"skims.DIST\",\n",
    "        \"x2\": 'skims.reverse(\"DIST\")',\n",
    "    },\n",
    "    hashing_level=2,\n",
    ")\n",
    "arr1 = flow1.load_dataframe()\n",
    "arr1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert (\n",
    "    (\n",
    "        arr1\n",
    "        == np.array(\n",
    "            [\n",
    "                [\n",
    "                    0.00000e00,\n",
    "                    0.00000e00,\n",
    "                    1.20000e-01,\n",
    "                    1.20000e-01,\n",
    "                    1.20000e-01,\n",
    "                    1.20000e-01,\n",
    "                    1.20000e-01,\n",
    "                ],\n",
    "                [\n",
    "                    4.74000e02,\n",
    "                    2.24676e05,\n",
    "                    2.40000e-01,\n",
    "                    3.70000e-01,\n",
    "                    3.70000e-01,\n",
    "                    2.40000e-01,\n",
    "                    3.70000e-01,\n",
    "                ],\n",
    "                [\n",
    "                    4.74000e02,\n",
    "                    2.24676e05,\n",
    "                    4.40000e-01,\n",
    "                    5.70000e-01,\n",
    "                    5.70000e-01,\n",
    "                    4.40000e-01,\n",
    "                    5.70000e-01,\n",
    "                ],\n",
    "                [\n",
    "                    1.52000e02,\n",
    "                    2.31040e04,\n",
    "                    3.70000e-01,\n",
    "                    2.40000e-01,\n",
    "                    2.40000e-01,\n",
    "                    3.70000e-01,\n",
    "                    2.40000e-01,\n",
    "                ],\n",
    "                [\n",
    "                    0.00000e00,\n",
    "                    0.00000e00,\n",
    "                    1.40000e-01,\n",
    "                    1.40000e-01,\n",
    "                    1.40000e-01,\n",
    "                    1.40000e-01,\n",
    "                    1.40000e-01,\n",
    "                ],\n",
    "                [\n",
    "                    4.74000e02,\n",
    "                    2.24676e05,\n",
    "                    2.80000e-01,\n",
    "                    2.80000e-01,\n",
    "                    2.80000e-01,\n",
    "                    2.80000e-01,\n",
    "                    2.80000e-01,\n",
    "                ],\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "    )\n",
    "    .all()\n",
    "    .all()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "assert skims1.digital_encoding.baggage([\"WLK_LOC_WLK_FAR\"]) == {\"joined_0_offsets\"}\n",
    "assert (\n",
    "    skims1.iat(\n",
    "        otaz=[0, 1, 2], dtaz=[0, 0, 0], time_period=[1, 1, 1], _name=\"WLK_LOC_WLK_FAR\"\n",
    "    ).to_series()\n",
    "    == [0, 152, 474]\n",
    ").all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Pandas Categorical Dtype\n",
    "\n",
    "Dictionary encoding is very similar to the approach used for the pandas Categorical dtype, and\n",
    "can be used to achieve some of the efficiencies of categorical data, even though xarray lacks\n",
    "a formal native categorical data representation.  Sharrow's `construct` function for creating\n",
    "Dataset objects will automatically use dictionary encoding for \"category\" data. \n",
    "\n",
    "To demonstrate, we'll load some household data and create a categorical data column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hh = sh.example_data.get_households()\n",
    "hh[\"income_grp\"] = pd.cut(\n",
    "    hh.income, bins=[-np.inf, 30000, 60000, np.inf], labels=[\"Low\", \"Mid\", \"High\"]\n",
    ")\n",
    "hh = hh[[\"income\", \"income_grp\"]]\n",
    "hh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hh.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "We'll then create a Dataset using construct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hh_dataset = sh.dataset.construct(hh[[\"income\", \"income_grp\"]])\n",
    "hh_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Note that the \"income\" variable remains an integer as expected, but the \"income_grp\" variable, \n",
    "which had been a \"category\" dtype in pandas, is now stored as an `int8`, giving the \n",
    "category _index_ of each element (it would be an `int16` or larger if needed, but that's\n",
    "not necessary with only 3 categories). The information about the labels for the categories is \n",
    "retained not in the data itself but in the `digital_encoding`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hh_dataset[\"income_grp\"].digital_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "assert hh_dataset[\"income_grp\"].dtype == \"int8\"\n",
    "assert hh_dataset[\"income_grp\"].digital_encoding.keys() == {\"dictionary\", \"ordered\"}\n",
    "assert all(\n",
    "    hh_dataset[\"income_grp\"].digital_encoding[\"dictionary\"]\n",
    "    == np.array([\"Low\", \"Mid\", \"High\"], dtype=\"<U4\")\n",
    ")\n",
    "assert hh_dataset[\"income_grp\"].digital_encoding[\"ordered\"] is True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "If you try to make the return trip to a pandas DataFrame using the regular \n",
    "`xarray.Dataset.to_pandas()` method, the details of the categorical nature\n",
    "of this variable are lost, and only the int8 index is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hh_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "But, if you use the `single_dim` accessor on the dataset provided by sharrow,\n",
    "the categorical dtype is restored correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hh_dataset.single_dim.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TESTING\n",
    "pd.testing.assert_frame_equal(hh_dataset.single_dim.to_pandas(), hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "Note that this automatic handling of categorical data only applies when constructing\n",
    "or deconstructing a dataset with a single dimension (i.e. the `index` is not a MultiIndex).\n",
    "Multidimensional datasets use the normal xarray processing, which will dump string\n",
    "categoricals back into python objects, which is bad news for high performance applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sh.dataset.construct(\n",
    "    hh[[\"income\", \"income_grp\"]].reset_index().set_index([\"HHID\", \"income\"])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
