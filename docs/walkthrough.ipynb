{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7998b969",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "\n",
    "This notebook provides a short walkthrough of some of the features of the `sharrow` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from io import StringIO\n",
    "\n",
    "import sharrow as sh\n",
    "sh.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f409525",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# check versions\n",
    "import packaging\n",
    "assert packaging.version.parse(sh.__version__) >= packaging.version.parse(\"2022.0\")\n",
    "assert packaging.version.parse(xr.__version__) >= packaging.version.parse(\"0.20.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085397c",
   "metadata": {},
   "source": [
    "## Example Data\n",
    "\n",
    "We'll begin by importing some example data to work with.  We'll be using \n",
    "some test data taken from the MTC example in the ActivitySim project, including \n",
    "tables of data for households and persons, as well as a set of \n",
    "skims containing transportation level of service information for travel around\n",
    "a tiny slice of San Francisco.\n",
    "\n",
    "The households and persons are typical tabular data, and \n",
    "each can be read in and stored as a `pandas.DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f4bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "households = sh.example_data.get_households()\n",
    "households.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a369bd3",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test households content\n",
    "assert len(households) == 5000\n",
    "assert \"income\" in households \n",
    "assert households.index.name == \"HHID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2db078",
   "metadata": {},
   "outputs": [],
   "source": [
    "persons = sh.example_data.get_persons()\n",
    "persons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef0b92",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert len(persons) == 8212\n",
    "assert \"household_id\" in persons\n",
    "assert persons.index.name == 'PERID'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077d2d46",
   "metadata": {},
   "source": [
    "The skims, on the other hand, are not just simple tabular data, but rather a \n",
    "multi-dimensional representation of the transportation system, indexed by origin.\n",
    "destination, and time of day. Rather than using a single DataFrame for this data,\n",
    "we store it as a multi-dimensional `xarray.Dataset` â€” or, more exactly, a \n",
    "`sharrow.Dataset`, which is a subclass from the xarray version that adds some \n",
    "useful features we'll see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b087c457",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims = sh.example_data.get_skims()\n",
    "skims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d254a",
   "metadata": {},
   "source": [
    "Suppose we're wanting to simulate a tour mode choice.  Normally we'd probably have\n",
    "run through a bunch of different models to generate these tours and their destinations\n",
    "first, but let's just skip that for now and make up some random data to work with.  We'll \n",
    "just randomly choose (with replacement) 100,000 people, and send them to 100,000 zones, with\n",
    "random outbound and inbound time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118af761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_tours(n_tours=100_000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n_zones = skims.dims['dtaz']\n",
    "    return pd.DataFrame({\n",
    "        'PERID': rng.choice(persons.index, size=n_tours),\n",
    "        'dest_taz_idx': rng.choice(n_zones, size=n_tours),\n",
    "        'out_time_period': rng.choice(skims.time_period, size=n_tours),\n",
    "        'in_time_period': rng.choice(skims.time_period, size=n_tours),\n",
    "    }).rename_axis(\"TOURIDX\")\n",
    "tours = random_tours()\n",
    "tours.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa20b5",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert tours.index.name == \"TOURIDX\"\n",
    "assert 0 in tours.head().dest_taz_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c1fcf",
   "metadata": {},
   "source": [
    "Of note in this table, we include include destination TAZ's by index (position) not \n",
    "label, so we can observe a TAZ index of `0` even though the first TAZ ID is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1cda9d",
   "metadata": {},
   "source": [
    "## Spec Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c3467",
   "metadata": {},
   "source": [
    "Now that we've got our tours to work with, we'll also need \n",
    "an expression \"spec\" file that defines the utility function\n",
    "terms and coefficients.  Following the ActivitySim format, we\n",
    "can write a mini-spec file as appears below.  Each line of this\n",
    "CSV file has an expression that can be evaluated in the context\n",
    "of the various tables and datasets shown above, plus a set of \n",
    "coefficients that apply for that expression across various modal \n",
    "alternatives (drive, walk, and transit in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236e25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_spec = \"\"\"\n",
    "Label,Expression,DRIVE,WALK,TRANSIT\n",
    "Drive Time,odt_skims['SOV_TIME'] + dot_skims['SOV_TIME'],-0.0134,,\n",
    "Transit IVT,(odt_skims['WLK_LOC_WLK_TOTIVT']/100 + dot_skims['WLK_LOC_WLK_TOTIVT']/100),,,-0.0134\n",
    "Transit Wait Time,short_i_wait_mult * ((odt_skims['WLK_LOC_WLK_IWAIT']/100).clip(upper=shortwait) + (dot_skims['WLK_LOC_WLK_IWAIT']/100).clip(upper=shortwait)),,,-0.0134\n",
    "Income,hh.income > 60000,,-0.2,\n",
    "Constant,1,,-0.4,-0.55\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245373d",
   "metadata": {},
   "source": [
    "We'll use pandas to load these values into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b49c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = pd.read_csv(StringIO(mini_spec), index_col='Label')\n",
    "spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd68402",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert spec.index.name == \"Label\"\n",
    "assert all(spec.columns == ['Expression', 'DRIVE', 'WALK', 'TRANSIT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8add2a36",
   "metadata": {},
   "source": [
    "## Data Trees and Flows\n",
    "\n",
    "Then, it's time to prepare our data.  We'll create a `DataTree`\n",
    "that defines the relationships among all the datasets we're working\n",
    "with.  This is a tree in the mathematical sense, with nodes referencing\n",
    "the datasets and edges representing the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4a2e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_mc = sh.DataTree(\n",
    "    tour=tours,\n",
    "    person=persons,\n",
    "    hh=households,\n",
    "    odt_skims=skims,\n",
    "    dot_skims=skims,\n",
    "    relationships=(\n",
    "        \"tour.PERID @ person.PERID\",\n",
    "        \"person.household_id @ hh.HHID\",\n",
    "        \"hh.TAZ @ odt_skims.otaz\",\n",
    "        \"tour.dest_taz_idx -> odt_skims.dtaz\",\n",
    "        \"tour.out_time_period @ odt_skims.time_period\",\n",
    "        \"tour.dest_taz_idx -> dot_skims.otaz\",\n",
    "        \"hh.TAZ @ dot_skims.dtaz\",\n",
    "        \"tour.in_time_period @ dot_skims.time_period\",\n",
    "    ),\n",
    "    extra_vars={\n",
    "        'short_i_wait_mult': 0.75,\n",
    "        'shortwait': 3.0,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ac0779",
   "metadata": {},
   "source": [
    "The first named dataset we include, `tour`, is by default the root node of this data tree.\n",
    "We then can define an arbitrary number of other named data nodes.  Here, we add `person`, `hh`,\n",
    "`odt_skims` and `odt_skims`.  Note that these last two are actually two different names for the\n",
    "same underlying dataset, and for each name we will next define a unique set of relationships.\n",
    "\n",
    "All data nodes in this tree are stored as `Dataset` objects. We can give a pandas DataFrame\n",
    "in this contructor instead, but it will be automatically converted into a one-dimension `Dataset`.\n",
    "The conversion is no-copy if possible (and it is usually possible) so no additional memory is\n",
    "consumed in the conversion.\n",
    "\n",
    "The `relationships` defines links of the data tree. Each relationship maps a particular variable\n",
    "in a named upstream dataset to a particular dimension of a named downstream dataset.  For example,\n",
    "`\"person.household_id @ hh.HHID\"` tells the tree that the `household_id` variable in the `person` \n",
    "dataset contains labels (`@`) that map to the `HHID` dimension of the `hh` dataset.\n",
    "\n",
    "In addition to mapping by label, we can also map by position, by using the `->` operator in the\n",
    "relationship string instead of `@`.  In the example above, we map the tour destination TAZ's in\n",
    "this manner, as the `dest_taz_idx` variable in the `tours` dataset contains positional references\n",
    "instead of labels.\n",
    "\n",
    "Lastly, out tree definition includes a few named constants, that are just fixed values defined\n",
    "in a seperate dictionary.\n",
    "\n",
    "Once we have defined our data tree, we can use it along with the `spec`, to compute the utility\n",
    "for various alternatives in the choice model.  Sharrow allows us to compile this utility function\n",
    "into a `Flow`, which can be reused for massive speed gains on later utility evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860a197",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mc = tree_mc.setup_flow(spec.Expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea77fad",
   "metadata": {},
   "source": [
    "To use a `Flow` for preparing the array of data that backs the utility\n",
    "function, we can call the `load()` method. The first time we call `load()`,\n",
    "it takes a (relatively) long time to evaluate, as the expressions are compiled\n",
    "and that compiled code is cached to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafff212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow_mc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef334f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test utility data\n",
    "actual = flow_mc.load()\n",
    "expected = np.array([[  9.4     ,  16.9572  ,   4.5     ,   0.      ,   1.      ],\n",
    "       [  9.32    ,  14.3628  ,   4.5     ,   1.      ,   1.      ],\n",
    "       [  7.62    ,  11.0129  ,   4.5     ,   1.      ,   1.      ],\n",
    "       [  4.25    ,   7.6692  ,   2.50065 ,   0.      ,   1.      ],\n",
    "       [  6.16    ,   8.2186  ,   3.387825,   0.      ,   1.      ],\n",
    "       [  4.86    ,   4.9288  ,   4.5     ,   0.      ,   1.      ],\n",
    "       [  1.07    ,   0.      ,   0.      ,   0.      ,   1.      ],\n",
    "       [  8.52    ,  11.615499,   3.260325,   0.      ,   1.      ],\n",
    "       [ 11.74    ,  16.2798  ,   3.440325,   0.      ,   1.      ],\n",
    "       [ 10.48    ,  13.3974  ,   3.942825,   0.      ,   1.      ]], dtype=np.float32)\n",
    "\n",
    "np.testing.assert_array_almost_equal(actual[:5], expected[:5])\n",
    "np.testing.assert_array_almost_equal(actual[-5:], expected[-5:])\n",
    "assert actual.shape == (len(tours), len(spec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558482f0",
   "metadata": {},
   "source": [
    "Subsequent calls to `load()` are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84c4ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow_mc.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f461d972",
   "metadata": {},
   "source": [
    "It's not faster because it's cached the data, but because it's cached the compiled code.\n",
    "We can swap out the `tour` node in the tree for a different set of (similarly formatted)\n",
    "tours, and re-evaluate at that fast speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c445e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tours_2 = random_tours(seed=43)\n",
    "tours_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afd1597",
   "metadata": {},
   "source": [
    "Note that the flow requires not just a base dataset but a whole DataTree to operate,\n",
    "so to re-evaluate with a new `tours` we need to make a DataTree with `replace_datasets`.\n",
    "Fortuntately, this operation is no-copy so it doesn't consume much memory.  If all the \n",
    "datasets in a tree are linked by position (instead of by label) this would be almost \n",
    "instantaneous, but since our example tree here has tours linked by label it takes just a\n",
    "moment to rebuild the linkages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e178b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_2 = tree_mc.replace_datasets(tour=tours_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdc918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow_mc.load(tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78a1be9",
   "metadata": {},
   "source": [
    "The load function also has some other features, like nicely formatting the output\n",
    "into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = flow_mc.load_dataframe()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff5806",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test df\n",
    "assert len(df) == len(tours)\n",
    "pd.testing.assert_index_equal(\n",
    "    df.columns, \n",
    "    pd.Index(['Drive Time', 'Transit IVT', 'Transit Wait Time', 'Income', 'Constant']),\n",
    ")\n",
    "expected_df_head = pd.read_csv(StringIO(''',Drive Time,Transit IVT,Transit Wait Time,Income,Constant\n",
    "0,9.4,16.9572,4.5,0.0,1.0\n",
    "1,9.32,14.3628,4.5,1.0,1.0\n",
    "2,7.62,11.0129,4.5,1.0,1.0\n",
    "3,4.25,7.6692,2.50065,0.0,1.0\n",
    "4,6.16,8.2186,3.387825,0.0,1.0'''), index_col=0).astype(np.float32)\n",
    "pd.testing.assert_frame_equal(df.head(), expected_df_head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b552d0",
   "metadata": {},
   "source": [
    "## Linear-in-Parameters Functions\n",
    "\n",
    "When the `spec` represents a linear-in-parameters utility function, the data \n",
    "we get out of the `load()` function represents one matrix in a dot-product, and\n",
    "the coefficients in the `spec` provide the other matrix.  We might look to \n",
    "use the efficient linear algebra algorithms embedded in `np.dot` to compute the\n",
    "utility, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4cdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = flow_mc.load()\n",
    "b = spec.iloc[:,1:].fillna(0).astype(np.float32).values\n",
    "np.dot(x, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783f218",
   "metadata": {},
   "source": [
    "But `sharrow` provides a substantially faster option, by embedding\n",
    "the dot product directly into the compiled code and never instantiating the\n",
    "full `x` array in memory at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfc9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time u = flow_mc.dot(b)\n",
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afc8e3",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test utility\n",
    "np.testing.assert_array_almost_equal(u, np.dot(x, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5791b",
   "metadata": {},
   "source": [
    "As before, the compiler runs only the first time we apply the this \n",
    "function with this structure, and subsequent runs are faster, even with\n",
    "different source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae41e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time flow_mc.dot(b, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e16b3",
   "metadata": {},
   "source": [
    "As for the plain `load` method, the `dot` method also has some formatted output versions.\n",
    "For example, the `dot_dataarray` returns a `DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869c2b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_mc.dot_dataarray(b, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8f94c",
   "metadata": {},
   "source": [
    "It works better if the coefficients are given as a DataArray too, so it \n",
    "can harvest dimension names and coordinates as appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = xr.DataArray(\n",
    "    spec.iloc[:,1:].fillna(0).astype(np.float32), \n",
    "    dims=('expressions','modes')\n",
    ")\n",
    "flow_mc.dot_dataarray(B, source=tree_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c0e441",
   "metadata": {},
   "source": [
    "## Multinomial Logit Simulation\n",
    "\n",
    "The next level of flow evaluation is made by treating the dot-product as a\n",
    "linear-in-parameters multinomial logit (MNL) utility function, and making simulated\n",
    "choices based on that model.  To do this, we'll need to provide the random\n",
    "draws as a function input (which also lets us attach any randomization engine\n",
    "we prefer, e.g. a reproducible random generator).  For this example, we'll \n",
    "create one random (uniform) draw for each tour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd943d",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_mc = np.random.default_rng(321).random(size=tree_mc.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb2c18",
   "metadata": {},
   "source": [
    "Given those draws, we use the `mnl_draws` method to build and apply a \n",
    "MNL simulator, which returns to us both the choices and the probability that\n",
    "was computed for each chosen alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "choices_mc, choice_probs_mc = flow_mc.mnl_draws(b, draws_mc)\n",
    "choices_mc, choice_probs_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69618e5c",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test mnl choices\n",
    "uz = np.exp(flow_mc.dot(b))\n",
    "uz = uz / uz.sum(1)[:,None]\n",
    "np.testing.assert_array_almost_equal(\n",
    "    uz[range(uz.shape[0]),choices_mc.ravel()],\n",
    "    choice_probs_mc.ravel(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266f2390",
   "metadata": {},
   "source": [
    "## Multi-Dimensional Analysis\n",
    "\n",
    "Let take a look at preparing some data for a workplace location choice simulation model.\n",
    "This is a different kind of model, and it will use differently shaped data. The decision \n",
    "makers (or \"choosers\") in this model will be the workers. The alternatives \n",
    "will be the various zones included in the land use table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ef07d",
   "metadata": {},
   "source": [
    "We didn't use the land use data previosly, so let's load it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca135ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "landuse = sh.example_data.get_land_use()\n",
    "landuse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48daa3a7",
   "metadata": {},
   "source": [
    "As noted above, the decision makers (or \"choosers\") in this model will be workers, \n",
    "who are only a subset of the persons data we looked at before.  We can identify workers from\n",
    "values 1 and 2 (full-time employed and part-time employed) in the `'pemploy'` attribute \n",
    "of the `persons` table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331a24d4",
   "metadata": {},
   "source": [
    "As we filter the persons table to just the workers, we will also rename the index from\n",
    "\"PERSONID\" to \"WORKERID\".  This renaming is important for sharrow, as it expects dimensions\n",
    "that have the same name to match, but the workers don't align directly with the persons \n",
    "anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b86b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "workers = persons.query(\"pemploy in [1,2]\").rename_axis(index='WORKERID')\n",
    "workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f95d17",
   "metadata": {},
   "source": [
    "For our workplace location choice model, we will want to link in data from our skims,\n",
    "which can tell us about travel times and costs.  Since we have not yet determined a \n",
    "time of day for each worker's work tours, we'll just use the `'AM'` skims for the outbound\n",
    "leg of a hypothetical work tour, and the `'PM'` skims for the return leg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "skims_am = skims.sel(time_period='AM')\n",
    "skims_pm = skims.sel(time_period='PM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7168de32",
   "metadata": {},
   "source": [
    "## Creating a DataTree Iteratively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80968843",
   "metadata": {},
   "source": [
    "The last step in getting ready for this model is building out the relationships between all\n",
    "this data we've prepared. We'll again use the `DataTree` class to do that, but this time \n",
    "we'll demostrate building the tree in stages.  First, we'll assign a\n",
    "base Dataset to be the root data for the tree. We can start by creating an otherwise empty `Dataset` indexed on the two dimensions we want to end up with for analysis, workers and zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68826ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = sh.Dataset.from_named_objects(\n",
    "    workers.index, \n",
    "    landuse.index,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824c3c0c",
   "metadata": {},
   "source": [
    "Since our base dataset has\n",
    "two dimensions, we can specify a dimension order when writing into\n",
    "a DataTree (the default is alphabetical order).\n",
    "This ordering will be applied to outputs from the flows later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec526ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dest = sh.DataTree(base=base, dim_order=('WORKERID', 'TAZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a4e36",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "# test tree_dest attributes\n",
    "assert tree_dest.dim_order == ('WORKERID', 'TAZ')\n",
    "assert tree_dest.shape == (4361, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49bd3f5",
   "metadata": {},
   "source": [
    "Then, we can progressively build our `DataTree` by adding additional data. \n",
    "Each new branch of the tree we want to add using the `add_dataset` command should have a \n",
    "unique name, a dataset being attached, and one or more relationship declarations\n",
    "that describe how the new data attaches.  For example, we can attach the `persons`\n",
    "data like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594e5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dest.add_dataset('person', persons, \"base.WORKERID @ person.PERID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145858c",
   "metadata": {},
   "source": [
    "The relationship definition here starts with a dotted name of some data \n",
    "dimension already in the tree, an `@` operator to indicating matching by\n",
    "label in that dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e270e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dest.add_dataset('landuse', landuse, \"base.TAZ @ landuse.TAZ\")\n",
    "tree_dest.add_dataset('hh', households, \"person.household_id @ hh.HHID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7546c0",
   "metadata": {},
   "source": [
    "Unlike in the mode choice example above, we've already filtered the \n",
    "time period dimensions of the skims to be morning and afternoon peak,\n",
    "so we simply attach the two different parts, linking relationships only\n",
    "for the remaining dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a256e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_dest.add_dataset(\n",
    "    'odskims', \n",
    "    skims_am, \n",
    "    relationships=(\n",
    "        \"hh.TAZ @ odskims.otaz\", \n",
    "        \"base.TAZ @ odskims.dtaz\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "tree_dest.add_dataset(\n",
    "    'doskims', \n",
    "    skims_pm, \n",
    "    relationships=(\n",
    "        \"base.TAZ @ doskims.otaz\",\n",
    "        \"hh.TAZ @ doskims.dtaz\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d24e888",
   "metadata": {},
   "source": [
    "## Dynamically Defined Flows \n",
    "\n",
    "Although it is convenient, especially when working with \n",
    "ActivitySim, it's not strictly necessary to employ a 'spec' file in csv format; \n",
    "a simple Python dictionary can also be used to setup a flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273996a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_dc = tree_dest.setup_flow({\n",
    "    'round_trip_dist': 'odskims.DIST + doskims.DIST',\n",
    "    'round_trip_dist_first_mile': 'clip(odskims.DIST, 0, 1) + clip(doskims.DIST, 0, 1)',\n",
    "    'round_trip_dist_addl_miles': 'clip(odskims.DIST-1, 0, None) + clip(doskims.DIST-1, 0, None)',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02235003",
   "metadata": {},
   "source": [
    "Loading from this flow is done the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = flow_dc.load()\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7aa608",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert arr.shape == (4361, 25, 3)\n",
    "expected = np.array([[[ 0.61,  0.61,  0.  ],\n",
    "        [ 0.28,  0.28,  0.  ],\n",
    "        [ 0.56,  0.56,  0.  ],\n",
    "        [ 0.53,  0.53,  0.  ],\n",
    "        [ 1.23,  1.23,  0.  ]],\n",
    "\n",
    "       [[ 1.19,  1.19,  0.  ],\n",
    "        [ 1.49,  1.49,  0.  ],\n",
    "        [ 1.88,  1.85,  0.03],\n",
    "        [ 1.36,  1.36,  0.  ],\n",
    "        [ 1.93,  1.93,  0.  ]],\n",
    "\n",
    "       [[ 1.19,  1.19,  0.  ],\n",
    "        [ 1.49,  1.49,  0.  ],\n",
    "        [ 1.88,  1.85,  0.03],\n",
    "        [ 1.36,  1.36,  0.  ],\n",
    "        [ 1.93,  1.93,  0.  ]],\n",
    "\n",
    "       [[ 0.24,  0.24,  0.  ],\n",
    "        [ 0.61,  0.61,  0.  ],\n",
    "        [ 1.01,  1.01,  0.  ],\n",
    "        [ 0.75,  0.75,  0.  ],\n",
    "        [ 1.38,  1.38,  0.  ]],\n",
    "\n",
    "       [[ 0.61,  0.61,  0.  ],\n",
    "        [ 0.28,  0.28,  0.  ],\n",
    "        [ 0.56,  0.56,  0.  ],\n",
    "        [ 0.53,  0.53,  0.  ],\n",
    "        [ 1.23,  1.23,  0.  ]]], dtype=np.float32)\n",
    "\n",
    "np.testing.assert_array_almost_equal(arr[:5, :5, :], expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f121385a",
   "metadata": {},
   "source": [
    "For the tour mode example above, the tours dataset had only one dimension (TOURIDX),\n",
    "and so the output of the load function had two dimensions (TOURIDX and expressions).\n",
    "In this example, the base dataset in the tree has two dimensions (workers and zones)\n",
    "and so the result from the basic `load` function has *three* dimensions (workers, zones, and expressions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13782036",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9de50ce",
   "metadata": {},
   "source": [
    "Just as we could neatly format the two-dimensional output above as a `pandas.DataFrame`,\n",
    "so too can we neatly format this three-dimensional output, as a `xarray.DataArray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfa88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_pretty = flow_dc.load_dataarray()\n",
    "arr_pretty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03bb0e22",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "assert isinstance(arr_pretty, xr.DataArray)\n",
    "assert arr_pretty.dims == ('WORKERID', 'TAZ', 'expressions')\n",
    "assert arr_pretty.shape == (4361, 25, 3)\n",
    "assert all(arr_pretty.expressions == np.array(['round_trip_dist', 'round_trip_dist_first_mile',\n",
    "       'round_trip_dist_addl_miles'], dtype='<U26'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce998b",
   "metadata": {},
   "source": [
    "## Dot-products with More Dimensions\n",
    "\n",
    "We can also use the `dot` method here with the two dimensional base.\n",
    "We'll apply a one-dimensional coefficients array, with length three to \n",
    "match the three terms in the spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504897a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = np.asarray([1.0, 0.1, 0.01])\n",
    "flow_dc.dot(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95acc3c",
   "metadata": {},
   "source": [
    "The `dot_dataarray` method does the same underlying computational work, but \n",
    "yields a well-formatted DataArray intead of just a plain numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae61561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_dc.dot_dataarray(coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff32a79",
   "metadata": {},
   "source": [
    "## More Logit Simulation\n",
    "\n",
    "And, just as above, we can build and simulate an MNL model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dec4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "draws_dc = np.random.default_rng(123).random(size=[4361, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f032eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_choices, dc_choice_probs = flow_dc.mnl_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws_dc,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1401fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4050bdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_choice_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e87f9f",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "expected = np.array(\n",
    "    [  [ 5,  8,  9, 18],\n",
    "       [ 6,  8, 19, 23],\n",
    "       [ 7,  9, 20, 22],\n",
    "       [ 8, 18, 18, 19],\n",
    "       [ 9, 17, 19, 20],\n",
    "       # ...,\n",
    "       [ 1, 12, 21, 22],\n",
    "       [ 0,  4, 21, 23],\n",
    "       [ 0,  3, 21, 21],\n",
    "       [ 3,  6, 22, 23],\n",
    "       [14, 14, 22, 23]], dtype=np.int32)\n",
    "np.testing.assert_array_equal(dc_choices[:5], expected[:5])\n",
    "np.testing.assert_array_equal(dc_choices[-5:], expected[-5:])\n",
    "\n",
    "expected = np.array([[ 0.021751,  0.081996,  0.09071 ,  0.239192],\n",
    "       [ 0.052467,  0.139753,  0.064863,  0.03138 ],\n",
    "       [ 0.063566,  0.103221,  0.021153,  0.05463 ],\n",
    "       [ 0.084491,  0.246472,  0.246472,  0.134457],\n",
    "       [ 0.09071 ,  0.082828,  0.130486,  0.038443],\n",
    "       ##...,\n",
    "       [ 0.039684,  0.020115,  0.087248,  0.207961],\n",
    "       [ 0.046644,  0.018624,  0.087248,  0.069162],\n",
    "       [ 0.046644,  0.025064,  0.087248,  0.087248],\n",
    "       [ 0.025064,  0.027071,  0.207961,  0.069162],\n",
    "       [ 0.048079,  0.048079,  0.207961,  0.069162]])\n",
    "np.testing.assert_array_almost_equal(dc_choice_probs[:5], expected[:5])\n",
    "np.testing.assert_array_almost_equal(dc_choice_probs[-5:], expected[-5:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700d084",
   "metadata": {},
   "source": [
    "It's more common to make many repeated choices for destination choice type models\n",
    "(e.g. to sample destinations), so there's also a \"pick count\" feature, that\n",
    "can summarize the simulation results efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d030f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dc_choices_, dc_choice_probs_, dc_pick_count = flow_dc.mnl_draws(\n",
    "    coefficients=coefs,\n",
    "    draws=draws_dc,\n",
    "    pick_counted=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a84dab",
   "metadata": {},
   "source": [
    "If you compare against the non-pick-counted results above, you'll see \n",
    "that we get exactly the same choices out, but when choices are repeated\n",
    "they are aggregated in the resulting arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb1a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate([dc_choices_, dc_pick_count], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9edee6",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "expected = np.array([[ 5,  8,  9, 18,  1,  1,  1,  1],\n",
    "       [ 6,  8, 19, 23,  1,  1,  1,  1],\n",
    "       [ 7,  9, 20, 22,  1,  1,  1,  1],\n",
    "       [ 8, 18, 19, -1,  1,  2,  1,  0],\n",
    "       [ 9, 17, 19, 20,  1,  1,  1,  1],\n",
    "       #...,\n",
    "       [ 1, 12, 21, 22,  1,  1,  1,  1],\n",
    "       [ 0,  4, 21, 23,  1,  1,  1,  1],\n",
    "       [ 0,  3, 21, -1,  1,  1,  2,  0],\n",
    "       [ 3,  6, 22, 23,  1,  1,  1,  1],\n",
    "       [14, 22, 23, -1,  2,  1,  1,  0]], dtype=np.int32)\n",
    "y = np.concatenate([dc_choices_, dc_pick_count], axis=1)\n",
    "np.testing.assert_array_equal(y[:5], expected[:5])\n",
    "np.testing.assert_array_equal(y[-5:], expected[-5:])\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
